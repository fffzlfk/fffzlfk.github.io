<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Cuda 编程模型 | fffzlfk's Blog</title><meta name=keywords content="CUDA"><meta name=description content="Kernels CUDA C++对C++进行了扩展，允许程序员定义C++函数，称为内核，当被调用时，由 $N$ 个不同的CUDA线程并行执行 $N$ 次，而不是像普通C++函数那样只执行一次。
kernel是使用__global__声明定义的，对于特定的内核调用，执行该内核的CUDA线程数量是使用<<<...>>>执行配置语法指定的（C++语言扩展）。每个执行内核的线程都有一个唯一的线程ID，可以在内核内通过内置变量访问。
作为说明，下面的示例代码，使用内置变量threadIdx，将两个大小为 $N$ 的向量 $A$ 和 $B$ 相加，并将结果存入向量 $C$ 。
// Kernel definition __global__ void VecAdd(float* A, float* B, float* C) { int i = threadIdx.x; C[i] = A[i] + B[i]; } int main() { ... // Kernel invocation with N threads VecAdd<<<1, N>>>(A, B, C); ... } 在这里，执行VecAdd()的 $N$ 个线程中的每一个都执行了一次加法。
线程体系 为方便起见，threadIdx是一个 $3$ 分量的向量，因此可以用一维、二维或三维的线程索引来识别线程，形成一个一维、二维或三维的线程块，称为线程块。这提供了一种自然的方式来调用域中的元素进行计算，如矢量、矩阵或体积。
一个例子，下面的代码将两个大小为 $N\times N$ 的矩阵 $A$ 和 $B$ 相加，并将结果存入矩阵 $C$ 。"><meta name=author content="fffzlfk"><link rel=canonical href=https://fffzlfk.github.io/posts/cuda_%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B/><link crossorigin=anonymous href=/assets/css/stylesheet.10e201bef5018cd178b810dc804320245217579bf45cf1a7bd64b0435fe19b7a.css integrity="sha256-EOIBvvUBjNF4uBDcgEMgJFIXV5v0XPGnvWSwQ1/hm3o=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.ab916b8151320cc693135f5df9fe4b75e6e4754d8f0dff5782df332cacd1ba8e.js integrity="sha256-q5FrgVEyDMaTE19d+f5LdebkdU2PDf9Xgt8zLKzRuo4=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://fffzlfk.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://fffzlfk.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://fffzlfk.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://fffzlfk.github.io/apple_touch_icon.png><link rel=mask-icon href=https://fffzlfk.github.io/favicon.ico><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css integrity=sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js integrity=sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><meta property="og:title" content="Cuda 编程模型"><meta property="og:description" content="Kernels CUDA C++对C++进行了扩展，允许程序员定义C++函数，称为内核，当被调用时，由 $N$ 个不同的CUDA线程并行执行 $N$ 次，而不是像普通C++函数那样只执行一次。
kernel是使用__global__声明定义的，对于特定的内核调用，执行该内核的CUDA线程数量是使用<<<...>>>执行配置语法指定的（C++语言扩展）。每个执行内核的线程都有一个唯一的线程ID，可以在内核内通过内置变量访问。
作为说明，下面的示例代码，使用内置变量threadIdx，将两个大小为 $N$ 的向量 $A$ 和 $B$ 相加，并将结果存入向量 $C$ 。
// Kernel definition __global__ void VecAdd(float* A, float* B, float* C) { int i = threadIdx.x; C[i] = A[i] + B[i]; } int main() { ... // Kernel invocation with N threads VecAdd<<<1, N>>>(A, B, C); ... } 在这里，执行VecAdd()的 $N$ 个线程中的每一个都执行了一次加法。
线程体系 为方便起见，threadIdx是一个 $3$ 分量的向量，因此可以用一维、二维或三维的线程索引来识别线程，形成一个一维、二维或三维的线程块，称为线程块。这提供了一种自然的方式来调用域中的元素进行计算，如矢量、矩阵或体积。
一个例子，下面的代码将两个大小为 $N\times N$ 的矩阵 $A$ 和 $B$ 相加，并将结果存入矩阵 $C$ 。"><meta property="og:type" content="article"><meta property="og:url" content="https://fffzlfk.github.io/posts/cuda_%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-01-19T12:55:19+08:00"><meta property="article:modified_time" content="2022-01-19T12:55:19+08:00"><meta property="og:site_name" content="fffzlfk's Blog"><meta name=twitter:card content="summary"><meta name=twitter:title content="Cuda 编程模型"><meta name=twitter:description content="Kernels CUDA C++对C++进行了扩展，允许程序员定义C++函数，称为内核，当被调用时，由 $N$ 个不同的CUDA线程并行执行 $N$ 次，而不是像普通C++函数那样只执行一次。
kernel是使用__global__声明定义的，对于特定的内核调用，执行该内核的CUDA线程数量是使用<<<...>>>执行配置语法指定的（C++语言扩展）。每个执行内核的线程都有一个唯一的线程ID，可以在内核内通过内置变量访问。
作为说明，下面的示例代码，使用内置变量threadIdx，将两个大小为 $N$ 的向量 $A$ 和 $B$ 相加，并将结果存入向量 $C$ 。
// Kernel definition __global__ void VecAdd(float* A, float* B, float* C) { int i = threadIdx.x; C[i] = A[i] + B[i]; } int main() { ... // Kernel invocation with N threads VecAdd<<<1, N>>>(A, B, C); ... } 在这里，执行VecAdd()的 $N$ 个线程中的每一个都执行了一次加法。
线程体系 为方便起见，threadIdx是一个 $3$ 分量的向量，因此可以用一维、二维或三维的线程索引来识别线程，形成一个一维、二维或三维的线程块，称为线程块。这提供了一种自然的方式来调用域中的元素进行计算，如矢量、矩阵或体积。
一个例子，下面的代码将两个大小为 $N\times N$ 的矩阵 $A$ 和 $B$ 相加，并将结果存入矩阵 $C$ 。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://fffzlfk.github.io/posts/"},{"@type":"ListItem","position":3,"name":"Cuda 编程模型","item":"https://fffzlfk.github.io/posts/cuda_%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Cuda 编程模型","name":"Cuda 编程模型","description":"Kernels CUDA C++对C++进行了扩展，允许程序员定义C++函数，称为内核，当被调用时，由 $N$ 个不同的CUDA线程并行执行 $N$ 次，而不是像普通C++函数那样只执行一次。\nkernel是使用__global__声明定义的，对于特定的内核调用，执行该内核的CUDA线程数量是使用\u0026lt;\u0026lt;\u0026lt;...\u0026gt;\u0026gt;\u0026gt;执行配置语法指定的（C++语言扩展）。每个执行内核的线程都有一个唯一的线程ID，可以在内核内通过内置变量访问。\n作为说明，下面的示例代码，使用内置变量threadIdx，将两个大小为 $N$ 的向量 $A$ 和 $B$ 相加，并将结果存入向量 $C$ 。\n// Kernel definition __global__ void VecAdd(float* A, float* B, float* C) { int i = threadIdx.x; C[i] = A[i] + B[i]; } int main() { ... // Kernel invocation with N threads VecAdd\u0026lt;\u0026lt;\u0026lt;1, N\u0026gt;\u0026gt;\u0026gt;(A, B, C); ... } 在这里，执行VecAdd()的 $N$ 个线程中的每一个都执行了一次加法。\n线程体系 为方便起见，threadIdx是一个 $3$ 分量的向量，因此可以用一维、二维或三维的线程索引来识别线程，形成一个一维、二维或三维的线程块，称为线程块。这提供了一种自然的方式来调用域中的元素进行计算，如矢量、矩阵或体积。\n一个例子，下面的代码将两个大小为 $N\\times N$ 的矩阵 $A$ 和 $B$ 相加，并将结果存入矩阵 $C$ 。","keywords":["CUDA"],"articleBody":"Kernels CUDA C++对C++进行了扩展，允许程序员定义C++函数，称为内核，当被调用时，由 $N$ 个不同的CUDA线程并行执行 $N$ 次，而不是像普通C++函数那样只执行一次。\nkernel是使用__global__声明定义的，对于特定的内核调用，执行该内核的CUDA线程数量是使用\u003c\u003c\u003c...\u003e\u003e\u003e执行配置语法指定的（C++语言扩展）。每个执行内核的线程都有一个唯一的线程ID，可以在内核内通过内置变量访问。\n作为说明，下面的示例代码，使用内置变量threadIdx，将两个大小为 $N$ 的向量 $A$ 和 $B$ 相加，并将结果存入向量 $C$ 。\n// Kernel definition __global__ void VecAdd(float* A, float* B, float* C) { int i = threadIdx.x; C[i] = A[i] + B[i]; } int main() { ... // Kernel invocation with N threads VecAdd\u003c\u003c\u003c1, N\u003e\u003e\u003e(A, B, C); ... } 在这里，执行VecAdd()的 $N$ 个线程中的每一个都执行了一次加法。\n线程体系 为方便起见，threadIdx是一个 $3$ 分量的向量，因此可以用一维、二维或三维的线程索引来识别线程，形成一个一维、二维或三维的线程块，称为线程块。这提供了一种自然的方式来调用域中的元素进行计算，如矢量、矩阵或体积。\n一个例子，下面的代码将两个大小为 $N\\times N$ 的矩阵 $A$ 和 $B$ 相加，并将结果存入矩阵 $C$ 。\n// Kernel definition __global__ void MatAdd(float A[N][N], float B[N][N], float C[N][N]) { int i = threadIdx.x; int j = threadIdx.y; C[i][j] = A[i][j] + B[i][j]; } int main() { ... // Kernel invocation with one block of N * N * 1 threads int numBlocks = 1; dim3 threadsPerBlock(N, N); MatAdd\u003c\u003c\u003cnumBlocks, threadsPerBlock\u003e\u003e\u003e(A, B, C); ... } 每个块的线程数量是有限制的，因为一个块的所有线程都要驻留在同一个处理器核心上，并且必须分享该核心的有限内存资源。在目前的GPU上，一个线程块最多可以包含 $1024$ 个线程。\n然而，一个内核可以由多个形状相同的线程块执行，因此线程的总数等于每个块的线程数乘以块的数量。\n块被组织成一个一维、二维或三维的线程块网格，如图所示。网格中的线程块的数量通常由正在处理的数据的大小决定，这通常超过了系统中的处理器数量。\n在\u003c\u003c\u003c...\u003e\u003e\u003e中指定的每个块的线程数和每个网格的块数可以是int或dim3类型。可以像上面的例子那样指定二维块或网格。\n网格中的每个块可以通过一个一维、二维或三维的唯一索引来识别，在内核中可以通过内置的blockIdx变量访问。线程块的尺寸可以在内核中通过内置的blockDim变量访问。\n扩展之前的MatAdd()例子以处理多个块，代码如下：\n// Kernel definition __global__ void MatAdd(float A[N][N], float B[N][N], float C[N][N]) { int i = blockIdx.x * blockDim.x + threadIdx.x; int j = blockIdx.y * blockDim.y + threadIdx.y; if (i \u003c N \u0026\u0026 j \u003c N) C[i][j] = A[i][j] + B[i][j]; } int main() { ... // Kernel invocation dim3 threadsPerBlock(16, 16); dim3 numBlocks(N / threadsPerBlock.x, N / threadsPerBlock.y); MatAdd\u003c\u003c\u003cnumBlocks, threadsPerBlock\u003e\u003e\u003e(A, B, C); ... } 线程块大小为 $16 \\times 16$ （ $256$ 个线程），尽管在这种情况下是任意的，但却是一种常见的选择。网格是用足够的块创建的，以便像以前一样每个矩阵元素有一个线程。为简单起见，本例假设每个维度的每个网格的线程数被该维度的每个块的线程数平均分割，尽管不一定是这样的。\n线程块被要求独立执行。必须能够以任何顺序、平行或串联的方式执行它们。如图所示，这种独立性要求允许线程块在任何数量的内核上以任何顺序进行调度，使程序员能够编写随内核数量扩展的代码。\n一个块内的线程可以通过一些共享内存来共享数据，并通过同步它们的执行来协调内存访问来进行合作。更确切地说，我们可以通过调用__syncthreads()函数来指定内核中的同步点；__syncthreads()作为一个障碍，块中的所有线程必须在这个障碍处等待，然后才允许继续进行。共享内存给出了一个使用共享内存的例子。除了__syncthreads()之外，合作组API还提供了一套丰富的线程同步原语。\n共享内存应该是靠近每个处理器核心的低延迟内存（很像L1高速缓存），并且__syncthreads()应该是轻量级的。\n内存体系 如图所示，CUDA线程在执行过程中可以从多个内存空间访问数据。每个线程都有私有的本地内存。每个线程块都有共享内存，对该块的所有线程都是可见的，并且与该块具有相同的生命周期。所有线程都可以访问相同的全局内存。\n还有两个额外的只读内存空间可供所有线程访问：常量和纹理内存空间。全局、常量和纹理内存空间针对不同的内存使用情况进行了优化（见设备内存访问）。纹理内存还为一些特定的数据格式提供了不同的寻址模式，以及数据过滤（见纹理和表面内存）。\n全局、常量和纹理内存空间在同一个应用程序启动内核时是持久的。\n异构编程 如图所示，CUDA编程模型假设CUDA线程在一个物理上独立的设备上执行，该设备作为运行C++程序的主机的协处理器运行。例如，当内核在GPU上执行，而C++程序的其余部分在CPU上执行时，就属于这种情况。\nCUDA编程模型还假定主机和设备都在DRAM中保持自己独立的内存空间，分别称为主机内存和设备内存。因此，程序通过调用CUDA运行时（在编程接口中描述）来管理内核可见的全局、常量和纹理内存空间。这包括设备内存的分配和撤销，以及主机和设备内存之间的数据传输。\n统一内存提供了管理的内存来连接主机和设备内存空间。管理的内存可以从系统中的所有CPU和GPU访问，作为一个具有共同地址空间的单一、连贯的内存图像。这种能力使设备内存的超额认购成为可能，并且通过消除在主机和设备上显式镜像数据的需要，大大简化了移植应用程序的任务。关于统一内存的介绍，请参见统一内存编程。\n异步的SIMT编程模型 在CUDA编程模型中，线程是进行计算或内存操作的最低级别的抽象概念。从基于NVIDIA Ampere GPU架构的设备开始，CUDA编程模型通过异步编程模型为内存操作提供加速。异步编程模型定义了与CUDA线程有关的异步操作的行为。\n异步编程模型定义了用于CUDA线程之间同步的异步障碍的行为。该模型还解释并定义了cuda::memcpy_async如何在GPU中计算时用于从全局内存异步移动数据。\n异步操作 异步操作被定义为由一个CUDA线程发起并由另一个线程异步执行的操作。在一个完善的程序中，一个或多个CUDA线程会与异步操作同步。启动异步操作的CUDA线程并不需要在同步线程中。\n这样的异步线程（as-if线程）总是与启动异步操作的CUDA线程有关。一个异步操作使用一个同步对象来同步完成操作。这样的同步对象可以由用户显式管理（如cuda::memcpy_async）或在库内隐式管理（如cooperative_groups::memcpy_async）。\n一个同步对象可以是cuda::barrier或cuda::pipeline。这些对象在Asynchronous Barrier和Asynchronous Data Copies using cuda::pipeline中有详细解释。这些同步对象可以在不同的线程范围内使用。一个范围定义了可以使用同步对象与异步操作同步的线程集合。下表定义了CUDA C++中可用的线程作用域以及可以与每个线程同步的线程。\nThread 作用域 描述 cuda::thread_scope::thread_scope_thread 只有发起异步操作的CUDA线程才会同步 cuda::thread_scope::thread_scope_block 与启动线程在同一线程块内的所有或任何CUDA线程都会同步 cuda::thread_scope::thread_scope_device 与启动线程相同的GPU设备中的所有或任何CUDA线程进行同步 cuda::thread_scope::thread_scope_system 与启动线程在同一系统中的所有或任何CUDA或CPU线程进行同步 这些线程作用域在CUDA Standard C++库中作为标准C++的扩展来实现。\nReferences https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#programming-model\n","wordCount":"250","inLanguage":"en","datePublished":"2022-01-19T12:55:19+08:00","dateModified":"2022-01-19T12:55:19+08:00","author":{"@type":"Person","name":"fffzlfk"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://fffzlfk.github.io/posts/cuda_%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B/"},"publisher":{"@type":"Organization","name":"fffzlfk's Blog","logo":{"@type":"ImageObject","url":"https://fffzlfk.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://fffzlfk.github.io accesskey=h title="fffzlfk's Blog (Alt + H)"><img src=https://fffzlfk.github.io/android-chrome-512x512.png alt aria-label=logo height=35>fffzlfk's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://github.com/fffzlfk title=GitHub><span>GitHub</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li><li><a href=https://fffzlfk.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://fffzlfk.github.io/archives title=Archive><span>Archive</span></a></li><li><a href=https://fffzlfk.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://www.travellings.cn/go.html title=Travellings><span>Travellings</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://fffzlfk.github.io>Home</a>&nbsp;»&nbsp;<a href=https://fffzlfk.github.io/posts/>Posts</a></div><h1 class=post-title>Cuda 编程模型</h1><div class=post-meta><span title='2022-01-19 12:55:19 +0800 +0800'>January 19, 2022</span>&nbsp;·&nbsp;2 min&nbsp;·&nbsp;fffzlfk&nbsp;|&nbsp;<a href=https://github.com/fffzlfk/fffzlfk.github.io/blob/master/content/posts/Cuda_%e7%bc%96%e7%a8%8b%e6%a8%a1%e5%9e%8b.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#kernels aria-label=Kernels>Kernels</a></li><li><a href=#%e7%ba%bf%e7%a8%8b%e4%bd%93%e7%b3%bb aria-label=线程体系>线程体系</a></li><li><a href=#%e5%86%85%e5%ad%98%e4%bd%93%e7%b3%bb aria-label=内存体系>内存体系</a></li><li><a href=#%e5%bc%82%e6%9e%84%e7%bc%96%e7%a8%8b aria-label=异构编程>异构编程</a></li><li><a href=#%e5%bc%82%e6%ad%a5%e7%9a%84simt%e7%bc%96%e7%a8%8b%e6%a8%a1%e5%9e%8b aria-label=异步的SIMT编程模型>异步的SIMT编程模型</a><ul><li><a href=#%e5%bc%82%e6%ad%a5%e6%93%8d%e4%bd%9c aria-label=异步操作>异步操作</a></li></ul></li><li><a href=#references aria-label=References>References</a></li></ul></div></details></div><div class=post-content><h2 id=kernels>Kernels<a hidden class=anchor aria-hidden=true href=#kernels>#</a></h2><p><code>CUDA C++</code>对<code>C++</code>进行了扩展，允许程序员定义<code>C++</code>函数，称为内核，当被调用时，由 $N$ 个不同的<code>CUDA</code>线程并行执行 $N$ 次，而不是像普通<code>C++</code>函数那样只执行一次。</p><p><code>kernel</code>是使用<code>__global__</code>声明定义的，对于特定的内核调用，执行该内核的<code>CUDA</code>线程数量是使用<code>&lt;&lt;&lt;...>>></code>执行配置语法指定的（<a href=https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#c-language-extensions>C++语言扩展</a>）。每个执行内核的线程都有一个唯一的线程<code>ID</code>，可以在内核内通过内置变量访问。</p><p>作为说明，下面的示例代码，使用内置变量<code>threadIdx</code>，将两个大小为 $N$ 的向量 $A$ 和 $B$ 相加，并将结果存入向量 $C$ 。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cpp data-lang=cpp><span style=display:flex><span><span style=color:#75715e>// Kernel definition
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>__global__ <span style=color:#66d9ef>void</span> <span style=color:#a6e22e>VecAdd</span>(<span style=color:#66d9ef>float</span><span style=color:#f92672>*</span> A, <span style=color:#66d9ef>float</span><span style=color:#f92672>*</span> B, <span style=color:#66d9ef>float</span><span style=color:#f92672>*</span> C)
</span></span><span style=display:flex><span>{
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>int</span> i <span style=color:#f92672>=</span> threadIdx.x;
</span></span><span style=display:flex><span>    C[i] <span style=color:#f92672>=</span> A[i] <span style=color:#f92672>+</span> B[i];
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>int</span> <span style=color:#a6e22e>main</span>()
</span></span><span style=display:flex><span>{
</span></span><span style=display:flex><span>    ...
</span></span><span style=display:flex><span>    <span style=color:#75715e>// Kernel invocation with N threads
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    VecAdd<span style=color:#f92672>&lt;&lt;&lt;</span><span style=color:#ae81ff>1</span>, N<span style=color:#f92672>&gt;&gt;&gt;</span>(A, B, C);
</span></span><span style=display:flex><span>    ...
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>在这里，执行<code>VecAdd()</code>的 $N$ 个线程中的每一个都执行了一次加法。</p><h2 id=线程体系>线程体系<a hidden class=anchor aria-hidden=true href=#线程体系>#</a></h2><p>为方便起见，<code>threadIdx</code>是一个 $3$ 分量的向量，因此可以用一维、二维或三维的线程索引来识别线程，形成一个一维、二维或三维的线程块，称为线程块。这提供了一种自然的方式来调用域中的元素进行计算，如矢量、矩阵或体积。</p><p>一个例子，下面的代码将两个大小为 $N\times N$ 的矩阵 $A$ 和 $B$ 相加，并将结果存入矩阵 $C$ 。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cpp data-lang=cpp><span style=display:flex><span><span style=color:#75715e>// Kernel definition
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>__global__ <span style=color:#66d9ef>void</span> <span style=color:#a6e22e>MatAdd</span>(<span style=color:#66d9ef>float</span> A[N][N], <span style=color:#66d9ef>float</span> B[N][N],
</span></span><span style=display:flex><span>                       <span style=color:#66d9ef>float</span> C[N][N])
</span></span><span style=display:flex><span>{
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>int</span> i <span style=color:#f92672>=</span> threadIdx.x;
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>int</span> j <span style=color:#f92672>=</span> threadIdx.y;
</span></span><span style=display:flex><span>    C[i][j] <span style=color:#f92672>=</span> A[i][j] <span style=color:#f92672>+</span> B[i][j];
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>int</span> <span style=color:#a6e22e>main</span>()
</span></span><span style=display:flex><span>{
</span></span><span style=display:flex><span>    ...
</span></span><span style=display:flex><span>    <span style=color:#75715e>// Kernel invocation with one block of N * N * 1 threads
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    <span style=color:#66d9ef>int</span> numBlocks <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span>;
</span></span><span style=display:flex><span>    dim3 threadsPerBlock(N, N);
</span></span><span style=display:flex><span>    MatAdd<span style=color:#f92672>&lt;&lt;&lt;</span>numBlocks, threadsPerBlock<span style=color:#f92672>&gt;&gt;&gt;</span>(A, B, C);
</span></span><span style=display:flex><span>    ...
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>每个块的线程数量是有限制的，因为一个块的所有线程都要驻留在同一个处理器核心上，并且必须分享该核心的有限内存资源。在目前的<code>GPU</code>上，一个线程块最多可以包含 $1024$ 个线程。</p><p>然而，一个内核可以由多个形状相同的线程块执行，因此线程的总数等于每个块的线程数乘以块的数量。</p><p>块被组织成一个一维、二维或三维的线程块网格，如图所示。网格中的线程块的数量通常由正在处理的数据的大小决定，这通常超过了系统中的处理器数量。</p><p><div class=render-image><img loading=lazy src=https://docs.nvidia.com/cuda/cuda-c-programming-guide/graphics/grid-of-thread-blocks.png alt></div></p><p>在<code>&lt;&lt;&lt;...>>></code>中指定的每个块的线程数和每个网格的块数可以是<code>int</code>或<code>dim3</code>类型。可以像上面的例子那样指定二维块或网格。</p><p>网格中的每个块可以通过一个一维、二维或三维的唯一索引来识别，在内核中可以通过内置的<code>blockIdx</code>变量访问。线程块的尺寸可以在内核中通过内置的<code>blockDim</code>变量访问。</p><p>扩展之前的<code>MatAdd()</code>例子以处理多个块，代码如下：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cpp data-lang=cpp><span style=display:flex><span><span style=color:#75715e>// Kernel definition
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>__global__ <span style=color:#66d9ef>void</span> <span style=color:#a6e22e>MatAdd</span>(<span style=color:#66d9ef>float</span> A[N][N], <span style=color:#66d9ef>float</span> B[N][N],
</span></span><span style=display:flex><span><span style=color:#66d9ef>float</span> C[N][N])
</span></span><span style=display:flex><span>{
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>int</span> i <span style=color:#f92672>=</span> blockIdx.x <span style=color:#f92672>*</span> blockDim.x <span style=color:#f92672>+</span> threadIdx.x;
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>int</span> j <span style=color:#f92672>=</span> blockIdx.y <span style=color:#f92672>*</span> blockDim.y <span style=color:#f92672>+</span> threadIdx.y;
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> (i <span style=color:#f92672>&lt;</span> N <span style=color:#f92672>&amp;&amp;</span> j <span style=color:#f92672>&lt;</span> N)
</span></span><span style=display:flex><span>        C[i][j] <span style=color:#f92672>=</span> A[i][j] <span style=color:#f92672>+</span> B[i][j];
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>int</span> <span style=color:#a6e22e>main</span>()
</span></span><span style=display:flex><span>{
</span></span><span style=display:flex><span>    ...
</span></span><span style=display:flex><span>    <span style=color:#75715e>// Kernel invocation
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    dim3 threadsPerBlock(<span style=color:#ae81ff>16</span>, <span style=color:#ae81ff>16</span>);
</span></span><span style=display:flex><span>    dim3 numBlocks(N <span style=color:#f92672>/</span> threadsPerBlock.x, N <span style=color:#f92672>/</span> threadsPerBlock.y);
</span></span><span style=display:flex><span>    MatAdd<span style=color:#f92672>&lt;&lt;&lt;</span>numBlocks, threadsPerBlock<span style=color:#f92672>&gt;&gt;&gt;</span>(A, B, C);
</span></span><span style=display:flex><span>    ...
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>线程块大小为 $16 \times 16$ （ $256$ 个线程），尽管在这种情况下是任意的，但却是一种常见的选择。网格是用足够的块创建的，以便像以前一样每个矩阵元素有一个线程。为简单起见，本例假设每个维度的每个网格的线程数被该维度的每个块的线程数平均分割，尽管不一定是这样的。</p><p>线程块被要求独立执行。必须能够以任何顺序、平行或串联的方式执行它们。如图所示，这种独立性要求允许线程块在任何数量的内核上以任何顺序进行调度，使程序员能够编写随内核数量扩展的代码。</p><p><div class=render-image><img loading=lazy src=https://docs.nvidia.com/cuda/cuda-c-programming-guide/graphics/automatic-scalability.png alt></div></p><p>一个块内的线程可以通过一些<strong>共享内存</strong>来共享数据，并通过同步它们的执行来协调内存访问来进行合作。更确切地说，我们可以通过调用<code>__syncthreads()</code>函数来指定内核中的同步点；<code>__syncthreads()</code>作为一个障碍，块中的所有线程必须在这个障碍处等待，然后才允许继续进行。共享内存给出了一个使用共享内存的例子。除了<code>__syncthreads()</code>之外，合作组<code>API</code>还提供了一套丰富的线程同步原语。</p><p>共享内存应该是靠近每个处理器核心的低延迟内存（很像<code>L1</code>高速缓存），并且<code>__syncthreads()</code>应该是轻量级的。</p><h2 id=内存体系>内存体系<a hidden class=anchor aria-hidden=true href=#内存体系>#</a></h2><p>如图所示，<code>CUDA</code>线程在执行过程中可以从多个内存空间访问数据。每个线程都有私有的本地内存。每个线程块都有共享内存，对该块的所有线程都是可见的，并且与该块具有相同的生命周期。所有线程都可以访问相同的<strong>全局</strong>内存。</p><p>还有两个额外的只读内存空间可供所有线程访问：<strong>常量</strong>和<strong>纹理</strong>内存空间。全局、常量和纹理内存空间针对不同的内存使用情况进行了优化（见<a href=https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses>设备内存访问</a>）。纹理内存还为一些特定的数据格式提供了不同的寻址模式，以及数据过滤（见<a href=https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#texture-and-surface-memory>纹理和表面内存</a>）。</p><p>全局、常量和纹理内存空间在同一个应用程序启动内核时是持久的。</p><p><div class=render-image><img loading=lazy src=https://docs.nvidia.com/cuda/cuda-c-programming-guide/graphics/memory-hierarchy.png alt></div></p><h2 id=异构编程>异构编程<a hidden class=anchor aria-hidden=true href=#异构编程>#</a></h2><p>如图所示，<code>CUDA</code>编程模型假设<code>CUDA</code>线程在一个物理上独立的设备上执行，该设备作为运行<code>C++</code>程序的主机的协处理器运行。例如，当内核在<code>GPU</code>上执行，而<code>C++</code>程序的其余部分在<code>CPU</code>上执行时，就属于这种情况。</p><p><div class=render-image><img loading=lazy src=https://docs.nvidia.com/cuda/cuda-c-programming-guide/graphics/heterogeneous-programming.png alt></div></p><p><code>CUDA</code>编程模型还假定主机和设备都在<code>DRAM</code>中保持自己独立的内存空间，分别称为主机内存和设备内存。因此，程序通过调用<code>CUDA</code>运行时（在<a href=https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#programming-interface>编程接口</a>中描述）来管理内核可见的全局、常量和纹理内存空间。这包括设备内存的分配和撤销，以及主机和设备内存之间的数据传输。</p><p>统一内存提供了管理的内存来连接主机和设备内存空间。管理的内存可以从系统中的所有<code>CPU</code>和<code>GPU</code>访问，作为一个具有共同地址空间的单一、连贯的内存图像。这种能力使设备内存的超额认购成为可能，并且通过消除在主机和设备上显式镜像数据的需要，大大简化了移植应用程序的任务。关于统一内存的介绍，请参见<a href=https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#um-unified-memory-programming-hd>统一内存编程</a>。</p><h2 id=异步的simt编程模型>异步的SIMT编程模型<a hidden class=anchor aria-hidden=true href=#异步的simt编程模型>#</a></h2><p>在<code>CUDA</code>编程模型中，线程是进行计算或内存操作的最低级别的抽象概念。从基于<code>NVIDIA Ampere GPU</code>架构的设备开始，<code>CUDA</code>编程模型通过异步编程模型为内存操作提供加速。异步编程模型定义了与CUDA线程有关的异步操作的行为。</p><p>异步编程模型定义了用于CUDA线程之间同步的异步障碍的行为。该模型还解释并定义了<code>cuda::memcpy_async</code>如何在GPU中计算时用于从全局内存异步移动数据。</p><h3 id=异步操作>异步操作<a hidden class=anchor aria-hidden=true href=#异步操作>#</a></h3><p>异步操作被定义为由一个<code>CUDA</code>线程发起并由另一个线程异步执行的操作。在一个完善的程序中，一个或多个<code>CUDA</code>线程会与异步操作同步。启动异步操作的<code>CUDA</code>线程并不需要在同步线程中。</p><p>这样的异步线程（<code>as-if</code>线程）总是与启动异步操作的<code>CUDA</code>线程有关。一个异步操作使用一个同步对象来同步完成操作。这样的同步对象可以由用户显式管理（如<code>cuda::memcpy_async</code>）或在库内隐式管理（如<code>cooperative_groups::memcpy_async</code>）。</p><p>一个同步对象可以是<code>cuda::barrier</code>或<code>cuda::pipeline</code>。这些对象在<a href=https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#aw-barrier>Asynchronous Barrier</a>和<a href=https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#memcpy_async_pipeline>Asynchronous Data Copies using cuda::pipeline</a>中有详细解释。这些同步对象可以在不同的线程范围内使用。一个范围定义了可以使用同步对象与异步操作同步的线程集合。下表定义了<code>CUDA C++</code>中可用的线程作用域以及可以与每个线程同步的线程。</p><table><thead><tr><th>Thread 作用域</th><th>描述</th></tr></thead><tbody><tr><td><code>cuda::thread_scope::thread_scope_thread</code></td><td>只有发起异步操作的CUDA线程才会同步</td></tr><tr><td><code>cuda::thread_scope::thread_scope_block</code></td><td>与启动线程在同一线程块内的所有或任何CUDA线程都会同步</td></tr><tr><td><code>cuda::thread_scope::thread_scope_device</code></td><td>与启动线程相同的GPU设备中的所有或任何CUDA线程进行同步</td></tr><tr><td><code>cuda::thread_scope::thread_scope_system</code></td><td>与启动线程在同一系统中的所有或任何CUDA或CPU线程进行同步</td></tr></tbody></table><p>这些线程作用域在<a href=https://nvidia.github.io/libcudacxx/extended_api/thread_scopes.html>CUDA Standard C++</a>库中作为标准C++的扩展来实现。</p><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><p><a href=https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#programming-model>https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#programming-model</a></p></div><footer class=post-footer><ul class=post-tags><li><a href=https://fffzlfk.github.io/tags/cuda/>CUDA</a></li></ul><nav class=paginav><a class=prev href=https://fffzlfk.github.io/posts/c++_%E5%AE%8C%E7%BE%8E%E8%BD%AC%E5%8F%91/><span class=title>« Prev</span><br><span>C++ 完美转发</span></a>
<a class=next href=https://fffzlfk.github.io/posts/cuda_%E7%A1%AC%E4%BB%B6%E5%AE%9E%E7%8E%B0/><span class=title>Next »</span><br><span>Cuda 硬件实现</span></a></nav></footer><div id=disqus_thread></div><script type=application/javascript>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//https-fffzlfk-github-io.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></article></main><footer class=footer><span>&copy; 2023 <a href=https://fffzlfk.github.io>fffzlfk's Blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>