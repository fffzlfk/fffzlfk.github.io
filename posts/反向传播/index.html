<!doctype html><html class="not-ready lg:text-base" style=--bg:#faf8f1 lang=zh-cn dir=ltr><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><title>反向传播 - fffzlfk's Blog</title>
<meta name=theme-color><meta name=description content="反向传播
反向传播（英语：Backpropagation，意为误差反向传播，缩写为BP）是对多层人工神经网络进行梯度下降的算法，也就是用链式法则以网络每层的权重为变数计算损失函数的梯度，以更新权重来最小化损失函数。
简单例子计算


  
      
          符号
          
      
  
  
      
          $ w_{ij} $
          权重
      
      
          $ z_i $
          输入
      
      
          $ y_i $
          输出
      
      
          $ E = \frac{1}{2}(y_p-y_a)^2$
          损失
      
      
          $ f(x) = \frac{1}{1+e^{-x}}$
          激活函数
      
  

例如更新$w_{53}$
主要思想就是我们无法找到$E$和$w_{53}$的直接关系，所以使用链式求导法则来间接求梯度：
$$
w_{53}(new) = w_{53}(old) - \Delta w_{53}
$$
$$
\begin{cases}
E = \frac{1}{2}(y_5-y_a)^2\\
y_5 = f(z_5)\\
z_5 = w_{53} * y_3 + w_{54} * y_4
\end{cases}
$$
$$
\begin{align}
\Delta w_{53} &= \frac{\partial E}{\partial w_{53}} \\
&= \frac{\partial E}{\partial y_{5}} \frac{\partial y_5}{\partial z_5} \frac{\partial z_5}{\partial w_{53}}
\end{align}
$$"><meta name=author content="fffzlfk"><link rel="preload stylesheet" as=style href=https://fffzlfk.github.io/main.min.css><link rel=preload as=image href=https://fffzlfk.github.io/theme.svg><link rel=preload as=image href="https://avatars.githubusercontent.com/u/44939690?v=4"><link rel=preload as=image href=https://fffzlfk.github.io/github.svg><link rel=preload as=image href=https://fffzlfk.github.io/instagram.svg><link rel=preload as=image href=https://fffzlfk.github.io/rss.svg><link rel=stylesheet href=/css/fonts.css><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.css integrity=sha384-3UiQGuEI4TTMaFmGIZumfRPtfKQ3trwQE2JgosJxCnGmQpL/lJdjpcHkaaFwHlcI crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.js integrity=sha384-G0zcxDFp5LWZtDuRMnBkk3EphCK1lhEf4UEyEM693ka574TZGwo4IWwS6QLzM/2t crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",()=>renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],throwOnError:!1}))</script><link rel=icon href=https://fffzlfk.github.io/favicon.ico><link rel=apple-touch-icon href=https://fffzlfk.github.io/apple-touch-icon.png><meta name=generator content="Hugo 0.147.4"><meta itemprop=name content="反向传播"><meta itemprop=description content="反向传播 反向传播（英语：Backpropagation，意为误差反向传播，缩写为BP）是对多层人工神经网络进行梯度下降的算法，也就是用链式法则以网络每层的权重为变数计算损失函数的梯度，以更新权重来最小化损失函数。
简单例子计算 符号 $ w_{ij} $ 权重 $ z_i $ 输入 $ y_i $ 输出 $ E = \frac{1}{2}(y_p-y_a)^2$ 损失 $ f(x) = \frac{1}{1+e^{-x}}$ 激活函数 例如更新$w_{53}$
主要思想就是我们无法找到$E$和$w_{53}$的直接关系，所以使用链式求导法则来间接求梯度： $$ w_{53}(new) = w_{53}(old) - \Delta w_{53} $$
$$ \begin{cases} E = \frac{1}{2}(y_5-y_a)^2\\ y_5 = f(z_5)\\ z_5 = w_{53} * y_3 + w_{54} * y_4 \end{cases} $$
$$ \begin{align} \Delta w_{53} &= \frac{\partial E}{\partial w_{53}} \\ &= \frac{\partial E}{\partial y_{5}} \frac{\partial y_5}{\partial z_5} \frac{\partial z_5}{\partial w_{53}} \end{align} $$"><meta itemprop=datePublished content="2022-10-18T21:52:39+08:00"><meta itemprop=dateModified content="2022-10-18T21:52:39+08:00"><meta itemprop=wordCount content="694"><meta itemprop=keywords content="NN,Python"><meta property="og:url" content="https://fffzlfk.github.io/posts/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/"><meta property="og:site_name" content="fffzlfk's Blog"><meta property="og:title" content="反向传播"><meta property="og:description" content="反向传播 反向传播（英语：Backpropagation，意为误差反向传播，缩写为BP）是对多层人工神经网络进行梯度下降的算法，也就是用链式法则以网络每层的权重为变数计算损失函数的梯度，以更新权重来最小化损失函数。
简单例子计算 符号 $ w_{ij} $ 权重 $ z_i $ 输入 $ y_i $ 输出 $ E = \frac{1}{2}(y_p-y_a)^2$ 损失 $ f(x) = \frac{1}{1+e^{-x}}$ 激活函数 例如更新$w_{53}$
主要思想就是我们无法找到$E$和$w_{53}$的直接关系，所以使用链式求导法则来间接求梯度： $$ w_{53}(new) = w_{53}(old) - \Delta w_{53} $$
$$ \begin{cases} E = \frac{1}{2}(y_5-y_a)^2\\ y_5 = f(z_5)\\ z_5 = w_{53} * y_3 + w_{54} * y_4 \end{cases} $$
$$ \begin{align} \Delta w_{53} &= \frac{\partial E}{\partial w_{53}} \\ &= \frac{\partial E}{\partial y_{5}} \frac{\partial y_5}{\partial z_5} \frac{\partial z_5}{\partial w_{53}} \end{align} $$"><meta property="og:locale" content="zh_cn"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-10-18T21:52:39+08:00"><meta property="article:modified_time" content="2022-10-18T21:52:39+08:00"><meta property="article:tag" content="NN"><meta property="article:tag" content="Python"><meta name=twitter:card content="summary"><meta name=twitter:title content="反向传播"><meta name=twitter:description content="反向传播 反向传播（英语：Backpropagation，意为误差反向传播，缩写为BP）是对多层人工神经网络进行梯度下降的算法，也就是用链式法则以网络每层的权重为变数计算损失函数的梯度，以更新权重来最小化损失函数。
简单例子计算 符号 $ w_{ij} $ 权重 $ z_i $ 输入 $ y_i $ 输出 $ E = \frac{1}{2}(y_p-y_a)^2$ 损失 $ f(x) = \frac{1}{1+e^{-x}}$ 激活函数 例如更新$w_{53}$
主要思想就是我们无法找到$E$和$w_{53}$的直接关系，所以使用链式求导法则来间接求梯度： $$ w_{53}(new) = w_{53}(old) - \Delta w_{53} $$
$$ \begin{cases} E = \frac{1}{2}(y_5-y_a)^2\\ y_5 = f(z_5)\\ z_5 = w_{53} * y_3 + w_{54} * y_4 \end{cases} $$
$$ \begin{align} \Delta w_{53} &= \frac{\partial E}{\partial w_{53}} \\ &= \frac{\partial E}{\partial y_{5}} \frac{\partial y_5}{\partial z_5} \frac{\partial z_5}{\partial w_{53}} \end{align} $$"><link rel=canonical href=https://fffzlfk.github.io/posts/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/></head><body class="bg-(--bg) text-black antialiased duration-200 ease-out [-webkit-tap-highlight-color:transparent] dark:text-white"><header class="mx-auto flex h-[4.5rem] max-w-(--w) px-8 whitespace-nowrap lg:justify-center"><div class="relative z-50 flex items-center ltr:mr-auto rtl:ml-auto"><a class="-translate-y-[1px] text-2xl font-medium" href=https://fffzlfk.github.io/>fffzlfk's Blog</a><div class="btn-dark text-[0px] ltr:ml-4 rtl:mr-4 h-6 w-6 shrink-0 cursor-pointer [background:url(./theme.svg)_left_center/cover_no-repeat] dark:invert dark:[background-position:right]" role=button aria-label=Dark></div></div><div class="btn-menu relative z-50 flex h-[4.5rem] w-[5rem] shrink-0 cursor-pointer flex-col items-center justify-center gap-2.5 lg:hidden ltr:-mr-8 rtl:-ml-8" role=button aria-label=Menu></div><script>const htmlClass=document.documentElement.classList;setTimeout(()=>{htmlClass.remove("not-ready")},10);const btnMenu=document.querySelector(".btn-menu");btnMenu.addEventListener("click",()=>{htmlClass.toggle("open")});const metaTheme=document.querySelector('meta[name="theme-color"]'),lightBg="#faf8f1".replace(/"/g,""),setDark=e=>{metaTheme.setAttribute("content",e?"#000":lightBg),htmlClass[e?"add":"remove"]("dark"),localStorage.setItem("dark",e)},darkScheme=window.matchMedia("(prefers-color-scheme: dark)");if(htmlClass.contains("dark"))setDark(!0);else{const e=localStorage.getItem("dark");setDark(e?e==="true":darkScheme.matches)}darkScheme.addEventListener("change",e=>{setDark(e.matches)});const btnDark=document.querySelector(".btn-dark");btnDark.addEventListener("click",()=>{setDark(localStorage.getItem("dark")!=="true")})</script><div class="nav-wrapper fixed inset-x-0 top-full z-40 flex h-full flex-col justify-center bg-(--bg) pb-16 duration-200 select-none lg:static lg:h-auto lg:flex-row lg:bg-transparent! lg:pb-0 lg:transition-none"><nav class="mt-12 flex justify-center space-x-10 lg:mt-0 lg:items-center ltr:lg:ml-14 rtl:space-x-reverse rtl:lg:mr-14 dark:invert"><a class="h-7 w-7 text-[0px] [background:var(--url)_center_center/cover_no-repeat] lg:h-6 lg:w-6" style=--url:url(./github.svg) href=https://github.com/fffzlfk target=_blank rel=me>github</a>
<a class="h-7 w-7 text-[0px] [background:var(--url)_center_center/cover_no-repeat] lg:h-6 lg:w-6" style=--url:url(./instagram.svg) href=https://instagram.com/fffzlfk target=_blank rel=me>instagram</a>
<a class="h-7 w-7 text-[0px] [background:var(--url)_center_center/cover_no-repeat] lg:h-6 lg:w-6" style=--url:url(./rss.svg) href=https://fffzlfk.github.io/index.xml target=_blank rel=alternate>rss</a></nav></div></header><main class="prose prose-neutral dark:prose-invert relative mx-auto min-h-[calc(100vh-9rem)] max-w-(--w) px-8 pt-14 pb-16"><article><header class=mb-14><h1 class="my-0! pb-2.5">反向传播</h1><div class="text-xs antialiased opacity-60"><time>Oct 18, 2022</time></div></header><section><h2 id=反向传播>反向传播</h2><p>反向传播（英语：Backpropagation，意为误差反向传播，缩写为BP）是对多层人工神经网络进行梯度下降的算法，也就是用链式法则以网络每层的权重为变数计算损失函数的梯度，以更新权重来最小化损失函数。</p><h2 id=简单例子计算>简单例子计算</h2><p><img src=https://p0.meituan.net/dpplatform/0102258668e345eee83a8459eaa8b95528740.png alt></p><table><thead><tr><th>符号</th><th></th></tr></thead><tbody><tr><td>$ w_{ij} $</td><td>权重</td></tr><tr><td>$ z_i $</td><td>输入</td></tr><tr><td>$ y_i $</td><td>输出</td></tr><tr><td>$ E = \frac{1}{2}(y_p-y_a)^2$</td><td>损失</td></tr><tr><td>$ f(x) = \frac{1}{1+e^{-x}}$</td><td>激活函数</td></tr></tbody></table><p>例如更新$w_{53}$</p><p>主要思想就是我们无法找到$E$和$w_{53}$的直接关系，所以使用链式求导法则来间接求梯度：
$$
w_{53}(new) = w_{53}(old) - \Delta w_{53}
$$</p><p>$$
\begin{cases}
E = \frac{1}{2}(y_5-y_a)^2\\
y_5 = f(z_5)\\
z_5 = w_{53} * y_3 + w_{54} * y_4
\end{cases}
$$</p><p>$$
\begin{align}
\Delta w_{53} &= \frac{\partial E}{\partial w_{53}} \\
&= \frac{\partial E}{\partial y_{5}} \frac{\partial y_5}{\partial z_5} \frac{\partial z_5}{\partial w_{53}}
\end{align}
$$</p><h3 id=代码实现>代码实现</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> math <span style=color:#f92672>import</span> exp, sqrt
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>f</span>(x):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;returns sigmoid(x)&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> <span style=color:#ae81ff>1</span> <span style=color:#f92672>/</span> (<span style=color:#ae81ff>1</span> <span style=color:#f92672>+</span> exp(<span style=color:#f92672>-</span>x))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>d_f</span>(x):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;return d(sigmoid(x))/dx&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> f(x) <span style=color:#f92672>*</span> (<span style=color:#ae81ff>1</span> <span style=color:#f92672>-</span> f(x))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>E</span>(yp, ya):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;return the error in squre&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> <span style=color:#ae81ff>1</span><span style=color:#f92672>/</span><span style=color:#ae81ff>2</span><span style=color:#f92672>*</span>(yp<span style=color:#f92672>-</span>ya)<span style=color:#f92672>**</span><span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>d_E</span>(yp, ya):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;return dE/d(yp)&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> yp <span style=color:#f92672>-</span> ya
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>ya <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.5</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>z <span style=color:#f92672>=</span> {<span style=color:#ae81ff>1</span>: <span style=color:#ae81ff>0.35</span>, <span style=color:#ae81ff>2</span>: <span style=color:#ae81ff>0.9</span>, <span style=color:#ae81ff>3</span>: <span style=color:#ae81ff>0.0</span>, <span style=color:#ae81ff>4</span>: <span style=color:#ae81ff>0.0</span>, <span style=color:#ae81ff>5</span>: <span style=color:#ae81ff>0.0</span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>w <span style=color:#f92672>=</span> {(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>3</span>): <span style=color:#ae81ff>0.1</span>, (<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>3</span>): <span style=color:#ae81ff>0.8</span>, (<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>4</span>): <span style=color:#ae81ff>0.4</span>,
</span></span><span style=display:flex><span>     (<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>4</span>): <span style=color:#ae81ff>0.6</span>, (<span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>5</span>): <span style=color:#ae81ff>0.3</span>, (<span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>5</span>): <span style=color:#ae81ff>0.9</span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>y <span style=color:#f92672>=</span> {i: <span style=color:#ae81ff>0.0</span> <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>6</span>)}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>s <span style=color:#f92672>=</span> {(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>3</span>): <span style=color:#ae81ff>0.0</span>, (<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>3</span>): <span style=color:#ae81ff>0.0</span>, (<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>4</span>): <span style=color:#ae81ff>0.0</span>,
</span></span><span style=display:flex><span>     (<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>4</span>): <span style=color:#ae81ff>0.0</span>, (<span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>5</span>): <span style=color:#ae81ff>0.0</span>, (<span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>5</span>): <span style=color:#ae81ff>0.0</span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>():
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;正向计算&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    y[<span style=color:#ae81ff>1</span>], y[<span style=color:#ae81ff>2</span>] <span style=color:#f92672>=</span> z[<span style=color:#ae81ff>1</span>], z[<span style=color:#ae81ff>2</span>]
</span></span><span style=display:flex><span>    z[<span style=color:#ae81ff>3</span>], z[<span style=color:#ae81ff>4</span>] <span style=color:#f92672>=</span> w[<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>3</span>] <span style=color:#f92672>*</span> y[<span style=color:#ae81ff>1</span>] <span style=color:#f92672>+</span> w[<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>3</span>] <span style=color:#f92672>*</span> \
</span></span><span style=display:flex><span>        y[<span style=color:#ae81ff>2</span>], w[<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>4</span>] <span style=color:#f92672>*</span> y[<span style=color:#ae81ff>1</span>] <span style=color:#f92672>+</span> w[<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>4</span>] <span style=color:#f92672>*</span> y[<span style=color:#ae81ff>2</span>]
</span></span><span style=display:flex><span>    y[<span style=color:#ae81ff>3</span>], y[<span style=color:#ae81ff>4</span>] <span style=color:#f92672>=</span> f(z[<span style=color:#ae81ff>3</span>]), f(z[<span style=color:#ae81ff>4</span>])
</span></span><span style=display:flex><span>    z[<span style=color:#ae81ff>5</span>] <span style=color:#f92672>=</span> w[<span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>5</span>] <span style=color:#f92672>*</span> y[<span style=color:#ae81ff>3</span>] <span style=color:#f92672>+</span> w[<span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>5</span>] <span style=color:#f92672>*</span> y[<span style=color:#ae81ff>4</span>]
</span></span><span style=display:flex><span>    y[<span style=color:#ae81ff>5</span>] <span style=color:#f92672>=</span> f(z[<span style=color:#ae81ff>5</span>])
</span></span><span style=display:flex><span>    e <span style=color:#f92672>=</span> E(y[<span style=color:#ae81ff>5</span>], ya)
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;yp = </span><span style=color:#e6db74>{</span>y[<span style=color:#ae81ff>5</span>]<span style=color:#e6db74>}</span><span style=color:#e6db74>, Error = </span><span style=color:#e6db74>{</span>e<span style=color:#e6db74>}</span><span style=color:#e6db74>&#39;</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> e
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>update</span>(n, g):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;update weights&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    w[n] <span style=color:#f92672>-=</span> g
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>back</span>():
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;back propagation&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    g <span style=color:#f92672>=</span> d_E(y[<span style=color:#ae81ff>5</span>], ya) <span style=color:#f92672>*</span> d_f(z[<span style=color:#ae81ff>5</span>]) <span style=color:#f92672>*</span> y[<span style=color:#ae81ff>3</span>]
</span></span><span style=display:flex><span>    update((<span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>5</span>), g)
</span></span><span style=display:flex><span>    g <span style=color:#f92672>=</span> d_E(y[<span style=color:#ae81ff>5</span>], ya) <span style=color:#f92672>*</span> d_f(z[<span style=color:#ae81ff>5</span>]) <span style=color:#f92672>*</span> y[<span style=color:#ae81ff>4</span>]
</span></span><span style=display:flex><span>    update((<span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>5</span>), g)
</span></span><span style=display:flex><span>    g <span style=color:#f92672>=</span> d_E(y[<span style=color:#ae81ff>5</span>], ya) <span style=color:#f92672>*</span> d_f(z[<span style=color:#ae81ff>5</span>]) <span style=color:#f92672>*</span> w[<span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>5</span>] <span style=color:#f92672>*</span> d_f(z[<span style=color:#ae81ff>3</span>]) <span style=color:#f92672>*</span> y[<span style=color:#ae81ff>1</span>]
</span></span><span style=display:flex><span>    update((<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>3</span>), g)
</span></span><span style=display:flex><span>    g <span style=color:#f92672>=</span> d_E(y[<span style=color:#ae81ff>5</span>], ya) <span style=color:#f92672>*</span> d_f(z[<span style=color:#ae81ff>5</span>]) <span style=color:#f92672>*</span> w[<span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>5</span>] <span style=color:#f92672>*</span> d_f(z[<span style=color:#ae81ff>3</span>]) <span style=color:#f92672>*</span> y[<span style=color:#ae81ff>2</span>]
</span></span><span style=display:flex><span>    update((<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>3</span>), g)
</span></span><span style=display:flex><span>    g <span style=color:#f92672>=</span> d_E(y[<span style=color:#ae81ff>5</span>], ya) <span style=color:#f92672>*</span> d_f(z[<span style=color:#ae81ff>5</span>]) <span style=color:#f92672>*</span> w[<span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>5</span>] <span style=color:#f92672>*</span> d_f(z[<span style=color:#ae81ff>4</span>]) <span style=color:#f92672>*</span> y[<span style=color:#ae81ff>1</span>]
</span></span><span style=display:flex><span>    update((<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>4</span>), g)
</span></span><span style=display:flex><span>    g <span style=color:#f92672>=</span> d_E(y[<span style=color:#ae81ff>5</span>], ya) <span style=color:#f92672>*</span> d_f(z[<span style=color:#ae81ff>5</span>]) <span style=color:#f92672>*</span> w[<span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>5</span>] <span style=color:#f92672>*</span> d_f(z[<span style=color:#ae81ff>4</span>]) <span style=color:#f92672>*</span> y[<span style=color:#ae81ff>2</span>]
</span></span><span style=display:flex><span>    update((<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>4</span>), g)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> __name__ <span style=color:#f92672>==</span> <span style=color:#e6db74>&#39;__main__&#39;</span>:
</span></span><span style=display:flex><span>    step <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>while</span> <span style=color:#66d9ef>True</span>:
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;step = </span><span style=color:#e6db74>{</span>step<span style=color:#e6db74>}</span><span style=color:#e6db74>:&#39;</span>, end<span style=color:#f92672>=</span><span style=color:#e6db74>&#39; &#39;</span>)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> forward() <span style=color:#f92672>&lt;</span> <span style=color:#ae81ff>1e-7</span>:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>break</span>
</span></span><span style=display:flex><span>        step <span style=color:#f92672>+=</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>        back()
</span></span></code></pre></div><p>运行代码，发现迭代111次后，损失才达到预期。</p><pre tabindex=0><code class=language-terminal data-lang=terminal>➜ python3 main.py
step = 1: yp = 0.6902834929076443, Error = 0.01810390383656677
step = 2: yp = 0.6820312027460466, Error = 0.016567679386586154
step = 3: yp = 0.6739592936119999, Error = 0.015130917916992996
step = 4: yp = 0.6660860653430867, Error = 0.013792290550574028
...
...
...
step = 110: yp = 0.500455314229855, Error = 1.036555239542297e-07
step = 111: yp = 0.5004302844701667, Error = 9.2572362633313e-08
</code></pre><h3 id=动量法优化>动量法优化</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>update</span>(n, g):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;update weights&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    lr <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.1</span>
</span></span><span style=display:flex><span>    beta <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.999</span>
</span></span><span style=display:flex><span>    epsilon <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.01</span>
</span></span><span style=display:flex><span>    s[n] <span style=color:#f92672>=</span> beta <span style=color:#f92672>*</span> s[n] <span style=color:#f92672>+</span> (<span style=color:#ae81ff>1</span> <span style=color:#f92672>-</span> beta) <span style=color:#f92672>*</span> g<span style=color:#f92672>**</span><span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>    w[n] <span style=color:#f92672>-=</span> lr <span style=color:#f92672>/</span> (sqrt(s[n]) <span style=color:#f92672>+</span> epsilon) <span style=color:#f92672>*</span> g
</span></span></code></pre></div><p>代码如上，使用动量法优化后，仅迭代11步损失就达到了预期。</p><pre tabindex=0><code class=language-terminal data-lang=terminal>➜ python3 main.py
step = 1: yp = 0.6902834929076443, Error = 0.01810390383656677
step = 2: yp = 0.6118790538896405, Error = 0.006258461349620539
step = 3: yp = 0.5593494607066376, Error = 0.0017611792430843605
step = 4: yp = 0.5301730589054645, Error = 0.00045520674185631563
step = 5: yp = 0.5151484953395415, Error = 0.00011473845552605596
step = 6: yp = 0.5075810359788381, Error = 2.873605325621872e-05
step = 7: yp = 0.5037909668833773, Error = 7.185714955431785e-06
step = 8: yp = 0.5018953359787233, Error = 1.7961492361214424e-06
step = 9: yp = 0.5009475298860605, Error = 4.48906442488917e-07
step = 10: yp = 0.5004736747645289, Error = 1.1218389127573745e-07
step = 11: yp = 0.5002367820786155, Error = 2.803287637675012e-08
</code></pre><h3 id=使用-pytorch-实现>使用 PyTorch 实现</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch.nn <span style=color:#66d9ef>as</span> nn
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>Net</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__init__</span>(self) <span style=color:#f92672>-&gt;</span> <span style=color:#66d9ef>None</span>:
</span></span><span style=display:flex><span>        super(Net, self)<span style=color:#f92672>.</span><span style=color:#a6e22e>__init__</span>()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        hidden <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>2</span>, bias<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span>        output <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>1</span>, bias<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span>        hidden<span style=color:#f92672>.</span>weight<span style=color:#f92672>.</span>data <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor([[<span style=color:#ae81ff>0.1</span>, <span style=color:#ae81ff>0.8</span>], [<span style=color:#ae81ff>0.4</span>, <span style=color:#ae81ff>0.6</span>]])
</span></span><span style=display:flex><span>        output<span style=color:#f92672>.</span>weight<span style=color:#f92672>.</span>data <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor([<span style=color:#ae81ff>0.3</span>, <span style=color:#ae81ff>0.9</span>])
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>net <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Sequential(
</span></span><span style=display:flex><span>            hidden,
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>Sigmoid(),
</span></span><span style=display:flex><span>            output,
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>Sigmoid()
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>net(x)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>MSE</span>(pred, act):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> <span style=color:#ae81ff>1</span><span style=color:#f92672>/</span><span style=color:#ae81ff>2</span><span style=color:#f92672>*</span>torch<span style=color:#f92672>.</span>mean((pred <span style=color:#f92672>-</span> act)<span style=color:#f92672>**</span><span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>device <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;cuda&#34;</span> <span style=color:#66d9ef>if</span> torch<span style=color:#f92672>.</span>cuda<span style=color:#f92672>.</span>is_available() <span style=color:#66d9ef>else</span> <span style=color:#e6db74>&#34;cpu&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>x <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor([<span style=color:#ae81ff>0.35</span>, <span style=color:#ae81ff>0.9</span>])<span style=color:#f92672>.</span>to(device)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>y <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor(<span style=color:#ae81ff>0.5</span>)<span style=color:#f92672>.</span>to(device)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>net <span style=color:#f92672>=</span> Net()<span style=color:#f92672>.</span>to(device)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>optimizer <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>optim<span style=color:#f92672>.</span>RMSprop(
</span></span><span style=display:flex><span>    net<span style=color:#f92672>.</span>parameters(), lr<span style=color:#f92672>=</span><span style=color:#ae81ff>0.1</span>, alpha<span style=color:#f92672>=</span><span style=color:#ae81ff>0.999</span>, eps<span style=color:#f92672>=</span><span style=color:#ae81ff>1e-2</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>train</span>():
</span></span><span style=display:flex><span>    optimizer<span style=color:#f92672>.</span>zero_grad()
</span></span><span style=display:flex><span>    pred <span style=color:#f92672>=</span> net(x)
</span></span><span style=display:flex><span>    loss <span style=color:#f92672>=</span> MSE(pred, y)
</span></span><span style=display:flex><span>    loss<span style=color:#f92672>.</span>backward()
</span></span><span style=display:flex><span>    optimizer<span style=color:#f92672>.</span>step()
</span></span><span style=display:flex><span>    err <span style=color:#f92672>=</span> loss<span style=color:#f92672>.</span>detach()<span style=color:#f92672>.</span>item()
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;pred = </span><span style=color:#e6db74>{</span>pred<span style=color:#e6db74>}</span><span style=color:#e6db74>, error = </span><span style=color:#e6db74>{</span>err<span style=color:#e6db74>}</span><span style=color:#e6db74>&#39;</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> err
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> __name__ <span style=color:#f92672>==</span> <span style=color:#e6db74>&#39;__main__&#39;</span>:
</span></span><span style=display:flex><span>    step <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>while</span> <span style=color:#66d9ef>True</span>:
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;step = </span><span style=color:#e6db74>{</span>step<span style=color:#e6db74>}</span><span style=color:#e6db74>:&#39;</span>, end<span style=color:#f92672>=</span><span style=color:#e6db74>&#39; &#39;</span>)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> train() <span style=color:#f92672>&lt;</span> <span style=color:#ae81ff>1e-7</span>:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>break</span>
</span></span><span style=display:flex><span>        step <span style=color:#f92672>+=</span> <span style=color:#ae81ff>1</span>
</span></span></code></pre></div><h2 id=references>References</h2><p><a href=https://zh.m.wikipedia.org/zh-cn/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95>https://zh.m.wikipedia.org/zh-cn/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95</a></p></section><footer class="mt-12 flex flex-wrap"><a class="mb-1.5 rounded-lg bg-black/[3%] px-5 py-1 no-underline hover:bg-black/[6%] ltr:mr-1.5 rtl:ml-1.5 dark:bg-white/[8%] dark:hover:bg-white/[12%]" href=https://fffzlfk.github.io/tags/nn>NN</a><a class="mb-1.5 rounded-lg bg-black/[3%] px-5 py-1 no-underline hover:bg-black/[6%] ltr:mr-1.5 rtl:ml-1.5 dark:bg-white/[8%] dark:hover:bg-white/[12%]" href=https://fffzlfk.github.io/tags/python>Python</a></footer><div class="giscus mt-24"></div><script src=https://giscus.app/client.js data-repo=fffzlfk/fffzlfk.github.io data-repo-id=R_kgDOG4ERGw data-category=Q&amp;A data-category-id=DIC_kwDOG4ERG84CBP5t data-mapping=pathname data-strict=1 data-reactions-enabled=0 data-emit-metadata=0 data-input-position=top data-theme=light data-lang=zh-CN data-loading=lazy crossorigin=anonymous async></script></article></main><footer class="mx-auto flex h-[4.5rem] max-w-(--w) items-center px-8 text-xs tracking-wider uppercase opacity-60"><div class=mr-auto>&copy;2025
<a class=link href=https://fffzlfk.github.io/>fffzlfk's Blog</a></div><a class="link mx-6" href=https://gohugo.io/ rel=noopener target=_blank>powered by hugo️️</a>️
<a class=link href=https://github.com/nanxiaobei/hugo-paper rel=noopener target=_blank>hugo-paper</a></footer></body></html>